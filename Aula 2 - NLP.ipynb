{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização e N-grama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "      'Eu gosto de assistir jogos de futebol',\n",
    "      'Já eu, prefiro assistir jogos de basquete'\n",
    "    ]\n",
    "    })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "exemplo = 'O futebol brasileiro é o melhor do mundo. Você concorda?'\n",
    "words = word_tokenize(exemplo)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = r\"(?<=@)[^.]+(?=\\.)\"\n",
    "re.findall(regex, \"dhenyt@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"dhenyt@gmail.com\"\n",
    "s.split(\"@\")[1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = 'O carro que estava quebrado voltou a funcionar'\n",
    "ex2 = 'Meu carro quebrou e não está funcionando'\n",
    "\n",
    "print(word_tokenize(ex1))\n",
    "print(word_tokenize(ex2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'text':[ex1,ex2]})\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "examples = [\n",
    "   \"go\",\"going\",\n",
    "   \"goes\",\"gone\",\"went\"\n",
    "]\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for word in examples:\n",
    "  print(wnl.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "doc = nlp(u'quebrou,quebraram,quebrado,quebrariam')\n",
    "[token.lemma_ for token in doc if token.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "examples = [\n",
    "    \"connection\",\"connections\",\n",
    "    \"connective\",\"connecting\",\"connected\"\n",
    "]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "examples = [\"conecta\",\"conectado\",\"conectamos\",\"desconectados\",\"conectividade\"]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\n",
    "examples = [\"conecta\",\"conectado\",\"conectamos\",\"desconectados\",\"conectividade\"]\n",
    "\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "for word in examples:\n",
    "  print(rslp.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem1 = \" \".join([rslp.stem(x) for x in word_tokenize(ex1)])\n",
    "stem2 = \" \".join([rslp.stem(x) for x in word_tokenize(ex2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':[stem1,stem2]})\n",
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.similar('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.similar('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('floresta')\n",
    "from nltk.corpus import floresta\n",
    "floresta.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_tag(t):\n",
    "  if \"+\" in t:\n",
    "    return t.split(\"+\")[1]\n",
    "  return t \n",
    "\n",
    "twords = nltk.corpus.floresta.tagged_words()\n",
    "twords = [(w.lower(),simplify_tag(t)) for (w,t) in twords]\n",
    "twords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.corpus.floresta.readme())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tag for (word, tag) in twords]\n",
    "nltk.FreqDist(tags).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'Esse é um exemplo utilizando o marcador padrão'\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "default_tagger = nltk.DefaultTagger('n')\n",
    "default_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsents = floresta.tagged_sents()\n",
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]\n",
    "train = tsents[1000:]\n",
    "test = tsents[:1000]\n",
    "tsents[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsents[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger0 = nltk.DefaultTagger('n')\n",
    "print(tagger0.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger1 = nltk.UnigramTagger(train)\n",
    "print(tagger1.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger2 = nltk.BigramTagger(train)\n",
    "print(tagger2.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinação de Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger1 = nltk.UnigramTagger(train, backoff=tagger0)\n",
    "print('tagger1: ',tagger1.evaluate(test))\n",
    "tagger2 = nltk.BigramTagger(train, backoff=tagger1)\n",
    "print('tagger2: ',tagger2.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pickle\n",
    "from pickle import dump\n",
    "output = open('tagger.pkl', 'wb')\n",
    "dump(tagger2, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "input = open('tagger.pkl', 'rb')\n",
    "tagger = load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Isso é para você.\"\n",
    "text2 = \"para com isso\"\n",
    "tokens1 = text1.split()\n",
    "tokens2 = text2.split()\n",
    "print('text1: ',tagger.tag(tokens1))\n",
    "print('text2: ',tagger.tag(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "opinion = TextBlob(\"This movie was horrible!\", analyzer=NaiveBayesAnalyzer())\n",
    "opinion.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'O estudo de processamento de linguagem natural é um dos campos mais promissores em inteligência artificial'\n",
    "print(\"Frase original: \", text)\n",
    "\n",
    "tblob = TextBlob(text)\n",
    "\n",
    "print('Idioma: ',tblob.detect_language())\n",
    "\n",
    "en_tblob = tblob.translate(to='en')\n",
    "\n",
    "print(\"Traduzido: \", en_tblob)\n",
    "\n",
    "print(\"Tags: \", en_tblob.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "doc = nlp(u'Ayrton Senna foi o melhor piloto de Fórmula 1 que já existiu')\n",
    "print([token.orth_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(token.orth_, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('filtrando apenas verbos: ')\n",
    "print([token.lemma_ for token in doc if token.pos_ == 'VERB'])\n",
    "print('identificação de entidades: ')\n",
    "doc1 = nlp(u'Machado de Assis um dos melhores escritores do Brasil, \\\n",
    "foi o primeiro presidente da Academia Brasileira de Letras')\n",
    "print(doc1.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
