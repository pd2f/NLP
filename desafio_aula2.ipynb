{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio - Aula 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://s3.amazonaws.com/automl-example/produtos.csv\",\n",
    "                 delimiter=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) utilizando o df acima carregado, faça:\n",
    "\n",
    "    -> Elimine linhas com valores nulos\n",
    "    -> Adicione uma nova coluna chamada texto, formada pela composição das colunas nome e descrição\n",
    "    -> Quantos Unigramas existem antes e depois de remover stopwords\n",
    "    -> Quantos Bigramas existem antes e depois de remover stopwords\n",
    "    -> Quantos Trigramas existem antes e depois de remover stopwords\n",
    "    -> Quantos verbos e Adverbios existem na nova coluna (utilize: from collections import Counter)\n",
    "    -> Quantos unigramas existem após aplicar Stemmer (utilize rslp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimine linhas com valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicione uma nova coluna chamada texto, formada pela composição das colunas nome e descrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pfranco\\AppData\\Local\\Continuum\\anaconda2\\envs\\py365\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_limpo['texto'] = df_limpo.nome +' '+ df_limpo.descricao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos Unigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes do stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df_limpo.texto)\n",
    "text_vect = vect.transform(df_limpo.texto)\n",
    "#pd.DataFrame(text_vect.A, columns=vect.get_feature_names()).T.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "São 506616 unigramas\n"
     ]
    }
   ],
   "source": [
    "print(\"São\",text_vect.sum(0).sum(1).A[0][0],\"unigramas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pfranco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stops = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois do stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_out_sword = CountVectorizer(ngram_range=(1,1),stop_words=stops)\n",
    "vect_out_sword.fit(df_limpo.texto)\n",
    "text_vect_out_sword = vect_out_sword.transform(df_limpo.texto)\n",
    "#pd.DataFrame(text_vect.A, columns=text_vect.get_feature_names()).T.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "São 365007 unigramas retirados os stopwords\n"
     ]
    }
   ],
   "source": [
    "print(\"São\",text_vect_out_sword.sum(0).sum(1).A[0][0],\"unigramas retirados os stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos Bigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramas = CountVectorizer(ngram_range=(2,2))\n",
    "bigramas.fit(df_limpo.texto)\n",
    "text_bigramas = bigramas.transform(df_limpo.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "São 503700 bigramas.\n"
     ]
    }
   ],
   "source": [
    "print(\"São\",text_bigramas.sum(0).sum(1).A[0][0],\"bigramas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois do stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramas_out_sword = CountVectorizer(ngram_range=(2,2),stop_words=stops)\n",
    "bigramas_out_sword.fit(df_limpo.texto)\n",
    "text_bigramas_out_sword = bigramas_out_sword.transform(df_limpo.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "São 362091 bigramas retirados os stopwords.\n"
     ]
    }
   ],
   "source": [
    "print(\"São\",text_bigramas_out_sword.sum(0).sum(1).A[0][0],\"bigramas retirados os stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos Trigramas existem antes e depois de remover stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramas = CountVectorizer(ngram_range=(3,3))\n",
    "trigramas.fit(df_limpo.texto)\n",
    "text_trigramas = trigramas.transform(df_limpo.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "São 500784 trigramas.\n"
     ]
    }
   ],
   "source": [
    "print(\"São\",text_trigramas.sum(0).sum(1).A[0][0],\"trigramas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois do stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramas_out_sword = CountVectorizer(ngram_range=(3,3),stop_words=stops)\n",
    "trigramas_out_sword.fit(df_limpo.texto)\n",
    "text_trigramas_out_sword = trigramas_out_sword.transform(df_limpo.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "São 359175 trigramas retirados os stopwords.\n"
     ]
    }
   ],
   "source": [
    "print(\"São\",text_trigramas_out_sword.sum(0).sum(1).A[0][0],\"trigramas retirados os stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade de Verbos e Advérbios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "qtd = Counter(['VERB','ADV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in df_limpo.texto:\n",
    "    doc = nlp(txt)\n",
    "    qtd +=Counter([token.pos_ for token in doc if token.pos_ in(['VERB','ADV'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de verbos: 105350 \n",
      "Quantidade de advérbios: 39595\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de verbos:\",qtd['VERB']-1,'\\n',\"\\rQuantidade de advérbios:\",qtd['ADV']-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantos unigramas existem após aplicar Stemmer (utilize rslp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\pfranco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.rslp import RSLPStemmer\n",
    "rslp = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pfranco\\AppData\\Local\\Continuum\\anaconda2\\envs\\py365\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for txt in df_limpo.texto:\n",
    "    df_limpo['texto_stem'] = rslp.stem(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df_limpo.texto_stem)\n",
    "text_vect = vect.transform(df_limpo.texto_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "São 597780 unigramas após aplicar Stemmer\n"
     ]
    }
   ],
   "source": [
    "print(\"São\",text_vect.sum(0).sum(1).A[0][0],\"unigramas após aplicar Stemmer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie uma variável que amazenará uma tupla (expressão, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2) crie um tagger baseado em expressões regulares:</h2>\n",
    "<br> -> crie uma variável que amazenará uma tupla (expressão, tag)\n",
    "<br>-> o tagger deverá capturar gerúndio (ando, endo, indo), plurais e números cardinais\n",
    "<br>-> utilize nltk.RegexpTagger(variável) para carregar seu tagger\n",
    "<br>-> apresente uma frase teste para cada tipo de expressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pfranco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerundio = r\"^[A-Za-z\\u00C0-\\u00FF]*ando(?:\\s|$)|^[A-Za-z\\u00C0-\\u00FF]*endo(?:\\s|$)|^[A-Za-z\\u00C0-\\u00FF]*indo(?:\\s|$)\"\n",
    "plural = r\"^[A-za-z\\u00C0-\\u00FF]*[Ss](?:\\s|$)\"\n",
    "cardinais = r\"[0-9]+\"\n",
    "frases = \"Hoje estava tranquilamente andando, caminhando sem pressa enquanto o ônibus estava vindo. Haviam duas propagandas neles: uma pessoa correndo e outra sorrindo. Nas últimas 12 horas ví essa propaganda 4 vezes.\"\n",
    "token = word_tokenize(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hoje', None),\n",
       " ('estava', None),\n",
       " ('tranquilamente', None),\n",
       " ('andando', 'gerundio'),\n",
       " (',', None),\n",
       " ('caminhando', 'gerundio'),\n",
       " ('sem', None),\n",
       " ('pressa', None),\n",
       " ('enquanto', None),\n",
       " ('o', None),\n",
       " ('ônibus', 'plural'),\n",
       " ('estava', None),\n",
       " ('vindo', 'gerundio'),\n",
       " ('.', None),\n",
       " ('Haviam', None),\n",
       " ('duas', 'plural'),\n",
       " ('propagandas', 'plural'),\n",
       " ('neles', 'plural'),\n",
       " (':', None),\n",
       " ('uma', None),\n",
       " ('pessoa', None),\n",
       " ('correndo', 'gerundio'),\n",
       " ('e', None),\n",
       " ('outra', None),\n",
       " ('sorrindo', 'gerundio'),\n",
       " ('.', None),\n",
       " ('Nas', 'plural'),\n",
       " ('últimas', 'plural'),\n",
       " ('12', 'cardinal'),\n",
       " ('horas', 'plural'),\n",
       " ('ví', None),\n",
       " ('essa', None),\n",
       " ('propaganda', None),\n",
       " ('4', 'cardinal'),\n",
       " ('vezes', 'plural'),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = nltk.RegexpTagger([(gerundio,'gerundio'),(plural,'plural'),(cardinais,'cardinal')])\n",
    "tagger.tag(nltk.word_tokenize(frases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=\"50\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Experiência complementar criando na mão as tags e utilizando o RegexpTagger com as tags criadas.</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag1 = [(str(x[0]),'gerundio') for x in [re.findall(gerundio,tk) for tk in token] if x != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2 = [(str(x[0]),'plural') for x in [re.findall(plural,tk) for tk in token] if x != []] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag3 = [(str(x[0]),'cardinal') for x in[re.findall(cardinais,tk) for tk in token] if x != []] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[]\n",
    "tags.extend(tag1)\n",
    "tags.extend(tag2)\n",
    "tags.extend(tag3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hoje', None),\n",
       " ('estava', None),\n",
       " ('tranquilamente', None),\n",
       " ('andando', 'gerundio'),\n",
       " (',', None),\n",
       " ('caminhando', 'gerundio'),\n",
       " ('sem', None),\n",
       " ('pressa', None),\n",
       " ('enquanto', None),\n",
       " ('o', None),\n",
       " ('ônibus', 'plural'),\n",
       " ('estava', None),\n",
       " ('vindo', 'gerundio'),\n",
       " ('.', None),\n",
       " ('Haviam', None),\n",
       " ('duas', 'plural'),\n",
       " ('propagandas', 'plural'),\n",
       " ('neles', 'plural'),\n",
       " (':', None),\n",
       " ('uma', None),\n",
       " ('pessoa', None),\n",
       " ('correndo', 'gerundio'),\n",
       " ('e', None),\n",
       " ('outra', None),\n",
       " ('sorrindo', 'gerundio'),\n",
       " ('.', None),\n",
       " ('Nas', 'plural'),\n",
       " ('últimas', 'plural'),\n",
       " ('12', 'cardinal'),\n",
       " ('horas', 'plural'),\n",
       " ('ví', None),\n",
       " ('essa', None),\n",
       " ('propaganda', None),\n",
       " ('4', 'cardinal'),\n",
       " ('vezes', 'plural'),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = nltk.RegexpTagger(tags)\n",
    "tagger.tag(nltk.word_tokenize(frases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
